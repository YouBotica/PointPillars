{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42732c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I can use GPU now? -- True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import ipdb\n",
    "\n",
    "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc7870",
   "metadata": {},
   "source": [
    "Load data from the KITTI dataset and perform train-test split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b209d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the point clouds.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_cloud_path = os.path.join(self.root_dir, self.files[idx])\n",
    "        point_cloud = self.load_point_cloud_from_bin(point_cloud_path)\n",
    "        return point_cloud\n",
    "    \n",
    "    def load_point_cloud_from_bin(self, bin_path):\n",
    "        with open(bin_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            point_cloud = np.frombuffer(content, dtype=np.float32)\n",
    "            point_cloud = point_cloud.reshape(-1, 4)  # KITTI point clouds are (x, y, z, intensity)\n",
    "        return torch.from_numpy(point_cloud)\n",
    "        \n",
    "\n",
    "train_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/velodyne_reduced'\n",
    "test_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/testing/velodyne_reduced'\n",
    "\n",
    "train_set = KITTIDataset(root_dir=train_dir)\n",
    "test_set = KITTIDataset(root_dir=test_dir)\n",
    "        \n",
    "        \n",
    "#batched_train_set = DataLoader(train_set, batch_size=4, shuffle=False)\n",
    "#batched_test_set = DataLoader(test_set, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d35975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading point cloud number 4591\n",
      "> \u001b[0;32m/tmp/ipykernel_597539/1872366522.py\u001b[0m(150)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    149 \u001b[0;31m            \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 150 \u001b[0;31m            \u001b[0mx_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpillarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# SUS: Is the indexing here correct?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    151 \u001b[0;31m            \u001b[0my_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpillarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(self.pillarizer.x_indices)\n",
      "19804\n",
      "ipdb> features.size()\n",
      "torch.Size([64, 16000])\n",
      "ipdb> features.shape[1]\n",
      "16000\n",
      "ipdb> self.pillarizr.x_indices.size()\n",
      "*** AttributeError: 'PseudoImageDataset' object has no attribute 'pillarizr'\n",
      "ipdb> self.pillarizer.x_indices.size()\n",
      "torch.Size([19804])\n",
      "ipdb> self.x_indices.size()\n",
      "*** AttributeError: 'PseudoImageDataset' object has no attribute 'x_indices'\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "class PillarFeatureNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PillarFeatureNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is of shape (D, P, N)\n",
    "        # Convert it to (P, D, N) for 1x1 convolution      \n",
    "        x = x.to(device)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # Max pooling operation over the points' dimension\n",
    "        x, _ = torch.max(x, dim=2)  # Output shape: (P, C)\n",
    "        return x.T  # Output shape: (C, P)\n",
    "\n",
    "\n",
    "\n",
    "class Pillarization:\n",
    "    def __init__(self, x_min, x_max, y_min, y_max, z_min, z_max, pillar_size, max_points_per_pillar, aug_dim):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.z_min = z_min\n",
    "        self.z_max = z_max\n",
    "        self.pillar_size = pillar_size\n",
    "        self.max_points_per_pillar = max_points_per_pillar\n",
    "        self.aug_dim = aug_dim\n",
    "        self.num_x_pillars = int((self.x_max - self.x_min) / self.pillar_size[0])\n",
    "        self.num_y_pillars = int((self.y_max - self.y_min) / self.pillar_size[1])\n",
    "        \n",
    "\n",
    "    def make_pillars(self, points):\n",
    "        \"\"\"\n",
    "        Convert point cloud (x, y, z) into pillars.\n",
    "        \"\"\"\n",
    "        # Mask points outside of our defined boundaries\n",
    "        \n",
    "        mask = (\n",
    "            (points[:, 0] >= self.x_min) & (points[:, 0] <= self.x_max) &\n",
    "            (points[:, 1] >= self.y_min) & (points[:, 1] <= self.y_max) &\n",
    "            (points[:, 2] >= self.z_min) & (points[:, 2] <= self.z_max)\n",
    "        )\n",
    "        points = points[mask]\n",
    "\n",
    "        \n",
    "        # Using numpy's digitize to find the interval/bin each point belongs to.\n",
    "        self.x_indices = torch.tensor(np.digitize(points[:, 0],\n",
    "            np.linspace(self.x_min, self.x_max, self.num_x_pillars))) - 1\n",
    "        \n",
    "        self.y_indices = torch.tensor(np.digitize(points[:, 1], \n",
    "            np.linspace(self.y_min, self.y_max, self.num_y_pillars))) - 1\n",
    "        \n",
    "        pillars = torch.zeros((self.num_x_pillars, self.num_y_pillars, self.max_points_per_pillar, \n",
    "            self.aug_dim))\n",
    "        \n",
    "        # Count how many points are in each pillar to ensure we don't exceed `max_points_per_pillar`\n",
    "        count = torch.zeros((self.num_x_pillars, self.num_y_pillars), dtype=torch.long)\n",
    "        \n",
    "        if (device != torch.device('cpu')):\n",
    "            self.x_indices.to(device)\n",
    "            self.y_indices.to(device)\n",
    "            points = points.to(device)\n",
    "            pillars = pillars.to(device)\n",
    "            count = count.to(device)\n",
    "           \n",
    "        # TODO: Calculate pillar x-y center:\n",
    "        pillar_x_center = self.x_indices * self.pillar_size[0] + self.pillar_size[0] / 2.0\n",
    "        pillar_y_center = self.y_indices * self.pillar_size[1] + self.pillar_size[1] / 2.0 \n",
    "\n",
    "        \n",
    "        # TODO: Store points in the pillars in a vectorized way filling the pillars tensor:        \n",
    "        for i in range(points.shape[0]):\n",
    "            x_ind = self.x_indices[i]\n",
    "            y_ind = self.y_indices[i]\n",
    "            \n",
    "            if count[x_ind, y_ind] < self.max_points_per_pillar:\n",
    "                # Compute x_c, y_c and z_c\n",
    "                x_c = (x_ind * self.pillar_size[0] + self.pillar_size[0] / 2.0) - points[i, 0]\n",
    "                y_c = (y_ind * self.pillar_size[1] + self.pillar_size[1] / 2.0) - points[i, 1]\n",
    "                z_c = (self.z_min + self.z_max) / 2 - points[i, 2] # assuming the z-center is the midpoint\n",
    "                \n",
    "                # Calculate pillar center\n",
    "                x_pillar_center = (x_ind * self.pillar_size[0] + self.pillar_size[0] / 2.0)\n",
    "                y_pillar_center = (y_ind * self.pillar_size[1] + self.pillar_size[1] / 2.0)\n",
    "                \n",
    "                # Add original x, y, and z coordinates, then x_c, y_c, z_c\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], :3] = points[i, :3]\n",
    "                \n",
    "                if (device != torch.device('cpu')): \n",
    "                    pillars[x_ind, y_ind, count[x_ind, y_ind], 3:6] = torch.tensor([x_c, y_c, z_c]).to(device)\n",
    "                else: \n",
    "                    pillars[x_ind, y_ind, count[x_ind, y_ind], 3:6] = torch.tensor([x_c, y_c, z_c])\n",
    "                    \n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], 6] = x_pillar_center - pillars[x_ind, y_ind, count[x_ind, y_ind], 0]\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], 7] = y_pillar_center - pillars[x_ind, y_ind, count[x_ind, y_ind], 1]\n",
    "                \n",
    "                count[x_ind, y_ind] += 1\n",
    "                \n",
    "        \n",
    "        # Zero-padding if too few point, random sampling if too many points:\n",
    "        for i in range(self.num_x_pillars):\n",
    "            for j in range(self.num_y_pillars):\n",
    "                if pillars[i, j].shape[0] > self.max_points_per_pillar:\n",
    "                    # Randomly sample points if there are too many for a given pillar\n",
    "                    pillars[i, j] = pillars[i, j][torch.randperm(pillars[i, j].shape[0])[:self.max_points_per_pillar]]\n",
    "                elif pillars[i, j].shape[0] < self.max_points_per_pillar:\n",
    "                    # Zero pad if there are too few points for a given pillar\n",
    "                    pillars[i, j] = torch.cat((pillars[i, j], torch.zeros((self.max_points_per_pillar - pillars[i, j].shape[0], pillars[i, j].shape[1]))))\n",
    "        \n",
    "        # Reshape pillars to size (D,P,N):\n",
    "        pillars = pillars.permute(3, 0, 1, 2).reshape(D, -1, N)\n",
    "        \n",
    "        return pillars\n",
    "\n",
    "\n",
    "class PseudoImageDataset(Dataset):\n",
    "    def __init__(self, train_dir, D, N, transform=None):\n",
    "        self.train_dir = train_dir\n",
    "        self.filenames = [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.pillarizer = Pillarization(aug_dim=D, x_min=-40.0, x_max=40.0, y_min=-25.0, y_max=25.0, \n",
    "                                        z_min=-3, z_max=3, pillar_size=(0.5, 0.5), max_points_per_pillar=N)\n",
    "        self.feature_extractor = PillarFeatureNet(D, 64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_cloud = train_set.load_point_cloud_from_bin(os.path.join(self.train_dir, self.filenames[idx]))\n",
    "        pillars = self.pillarizer.make_pillars(point_cloud)\n",
    "        \n",
    "        # Apply linear activation, batchnorm, and ReLU for feature extraction from pillars tensor\n",
    "        features = self.feature_extractor(pillars)\n",
    "        \n",
    "        # Generate pseudo-image (GPU)\n",
    "        pseudo_image = torch.zeros(features.shape[0], self.pillarizer.num_y_pillars, self.pillarizer.num_x_pillars).to(device)\n",
    "        \n",
    "        # Scatter the features back to their original pillar locations\n",
    "        print(f'Loading point cloud number {idx}')\n",
    "        for i in range(features.shape[1]):\n",
    "            ipdb.set_trace()\n",
    "            x_ind = self.pillarizer.x_indices[i].long() # SUS: Is the indexing here correct?\n",
    "            y_ind = self.pillarizer.y_indices[i].long()\n",
    "            pseudo_image[:, y_ind, x_ind] = features[:, i]\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            pseudo_image = self.transform(pseudo_image)\n",
    "            \n",
    "        return pseudo_image\n",
    "        \n",
    "        \n",
    "# Create the dataset and DataLoader\n",
    "D = 9\n",
    "N = 100\n",
    "dataset = PseudoImageDataset(train_dir=train_dir, D=D, N=N)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for batch_idx, (images) in enumerate(train_loader):\n",
    "    print(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf14b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(Dataset):\n",
    "    def __init__(self, root_dir, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the point clouds.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]\n",
    "            \n",
    "        if debug:\n",
    "            random_sample = random.randint(1, 7400)\n",
    "            self.files = [self.files[random_sample]]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_cloud_path = os.path.join(self.root_dir, self.files[idx])\n",
    "        point_cloud = self.load_point_cloud_from_bin(idx, point_cloud_path)  # Assuming point clouds are stored as .pt files\n",
    "        return point_cloud\n",
    "    \n",
    "    def load_point_cloud_from_bin(self, idx, root_dir):\n",
    "        idx_as_string = str(idx)\n",
    "        shift = len(idx_as_string)\n",
    "        file = '000000'\n",
    "        for i in range(len(file) - shift):\n",
    "            file[i] = idx_as_string[i-len(file)]\n",
    "                    \n",
    "        bin_path = os.path.join(root_dir, )\n",
    "        with open(bin_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "            point_cloud = np.frombuffer(content, dtype=np.float32)\n",
    "            point_cloud = point_cloud.reshape(-1, 4)  # KITTI point clouds are (x, y, z, intensity)\n",
    "        return torch.from_numpy(point_cloud)\n",
    "        \n",
    "\n",
    "train_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/velodyne_reduced'\n",
    "test_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/testing/velodyne_reduced'\n",
    "# TODO: Add label dir\n",
    "\n",
    "train_set = KITTIDataset(root_dir=train_dir, debug=False)\n",
    "test_set = KITTIDataset(root_dir=test_dir, debug=False)\n",
    "\n",
    "batched_train_set = DataLoader(train_set, batch_size=4, shuffle=False)\n",
    "batched_test_set = DataLoader(test_set, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c65a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790c2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PillarFeatureNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PillarFeatureNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is of shape (D, P, N)\n",
    "        # Convert it to (P, D, N) for 1x1 convolution\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # Max pooling operation over the points' dimension\n",
    "        x, _ = torch.max(x, dim=2)  # Output shape: (P, C)\n",
    "        return x.T  # Output shape: (C, P)\n",
    "\n",
    "\n",
    "\n",
    "class Pillarization:\n",
    "    def __init__(self, x_min, x_max, y_min, y_max, z_min, z_max, pillar_size, max_points_per_pillar, aug_dim):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.z_min = z_min\n",
    "        self.z_max = z_max\n",
    "        self.pillar_size = pillar_size\n",
    "        self.max_points_per_pillar = max_points_per_pillar\n",
    "        self.aug_dim = aug_dim\n",
    "        self.pillars_dict = {} # DEPRECATE?\n",
    "        self.num_x_pillars = int((self.x_max - self.x_min) / self.pillar_size[0])\n",
    "        self.num_y_pillars = int((self.y_max - self.y_min) / self.pillar_size[1])\n",
    "        \n",
    "\n",
    "    def make_pillars(self, points):\n",
    "        \"\"\"\n",
    "        Convert point cloud (x, y, z) into pillars.\n",
    "        \"\"\"\n",
    "        # Mask points outside of our defined boundaries\n",
    "        \n",
    "        mask = (\n",
    "            (points[:, 0] >= self.x_min) & (points[:, 0] <= self.x_max) &\n",
    "            (points[:, 1] >= self.y_min) & (points[:, 1] <= self.y_max) &\n",
    "            (points[:, 2] >= self.z_min) & (points[:, 2] <= self.z_max)\n",
    "        )\n",
    "        points = points[mask]\n",
    "\n",
    "        \n",
    "        # Using numpy's digitize to find the interval/bin each point belongs to.\n",
    "        # TODO: Get rid of these unnecessary copies of tensors\n",
    "        self.x_indices = torch.tensor(np.digitize(points[:, 0], np.linspace(self.x_min, self.x_max, self.num_x_pillars))) - 1\n",
    "        self.y_indices = torch.tensor(np.digitize(points[:, 1], np.linspace(self.y_min, self.y_max, self.num_y_pillars))) - 1\n",
    "\n",
    "        # TODO: Calculate pillar x-y center:\n",
    "        pillar_x_center = self.x_indices * self.pillar_size[0] + self.pillar_size[0] / 2.0\n",
    "        pillar_y_center = self.y_indices * self.pillar_size[1] + self.pillar_size[1] / 2.0 \n",
    "\n",
    "        pillars = torch.zeros((self.num_x_pillars, self.num_y_pillars, self.max_points_per_pillar, self.aug_dim))\n",
    "        \n",
    "        # Count how many points are in each pillar to ensure we don't exceed `max_points_per_pillar`\n",
    "        count = torch.zeros((self.num_x_pillars, self.num_y_pillars), dtype=torch.long)\n",
    "        \n",
    "        # TODO: Store points in the pillars in a vectorized way filling the pillars tensor:        \n",
    "        for i in range(points.shape[0]):\n",
    "            x_ind = self.x_indices[i]\n",
    "            y_ind = self.y_indices[i]\n",
    "            \n",
    "            if count[x_ind, y_ind] < self.max_points_per_pillar:\n",
    "                # Compute x_c, y_c and z_c\n",
    "                x_c = (x_ind * self.pillar_size[0] + self.pillar_size[0] / 2.0) - points[i, 0]\n",
    "                y_c = (y_ind * self.pillar_size[1] + self.pillar_size[1] / 2.0) - points[i, 1]\n",
    "                z_c = (self.z_min + self.z_max) / 2 - points[i, 2] # assuming the z-center is the midpoint\n",
    "                \n",
    "                # Calculate pillar center\n",
    "                x_pillar_center = (x_ind * self.pillar_size[0] + self.pillar_size[0] / 2.0)\n",
    "                y_pillar_center = (y_ind * self.pillar_size[1] + self.pillar_size[1] / 2.0)\n",
    "                \n",
    "                # Add original x, y, and z coordinates, then x_c, y_c, z_c\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], :3] = points[i, :3]\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], 3:6] = torch.tensor([x_c, y_c, z_c]) # Sus\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], 6] = x_pillar_center - pillars[x_ind, y_ind, count[x_ind, y_ind], 0]\n",
    "                pillars[x_ind, y_ind, count[x_ind, y_ind], 7] = y_pillar_center - pillars[x_ind, y_ind, count[x_ind, y_ind], 1]\n",
    "                \n",
    "                count[x_ind, y_ind] += 1\n",
    "        \n",
    "        # Zero-padding if too few point, random sampling if too many points:\n",
    "        for i in range(self.num_x_pillars):\n",
    "            for j in range(self.num_y_pillars):\n",
    "                if pillars[i, j].shape[0] > self.max_points_per_pillar:\n",
    "                    # Randomly sample points if there are too many for a given pillar\n",
    "                    pillars[i, j] = pillars[i, j][torch.randperm(pillars[i, j].shape[0])[:self.max_points_per_pillar]]\n",
    "                elif pillars[i, j].shape[0] < self.max_points_per_pillar:\n",
    "                    # Zero pad if there are too few points for a given pillar\n",
    "                    pillars[i, j] = torch.cat((pillars[i, j], torch.zeros((self.max_points_per_pillar - pillars[i, j].shape[0], pillars[i, j].shape[1]))))\n",
    "        \n",
    "        # Reshape pillars to size (D,P,N):\n",
    "        pillars = pillars.permute(3, 0, 1, 2).reshape(D, -1, N)\n",
    "        \n",
    "        return pillars\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
