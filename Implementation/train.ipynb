{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Can I can use GPU now? -- True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import ipdb\n",
    "\n",
    "# Pipelines (a.k.a parts of the Neural Network)\n",
    "from Pipelines.kitti_loader import KITTIDataset\n",
    "from Pipelines.pillarizer import PillarFeatureNet, Pillarization, PseudoImageDataset\n",
    "from Pipelines.backbone import BackBone\n",
    "from Pipelines.detection_head import DetectionHead\n",
    "from Pipelines.anchors import Box2D, Anchor\n",
    "from Pipelines.loss import PointPillarLoss\n",
    "from Pipelines.network import PointPillarsModel\n",
    "\n",
    "from Utils.transformations import transform_to_canvas, transform_to_grid, map_to_img\n",
    "from Utils.iou import calculate_iou\n",
    "from Utils.collate import normalize_annotations\n",
    "from Utils.boxes import create_boxes_tensor # FIXME: Should be in visualization instead\n",
    "\n",
    "# Visualization tools:\n",
    "from Visualization.visz_pointcloud_w_label import plot_point_cloud_with_bboxes_o3d\n",
    "from Visualization.visz_bboxes import visualize_batch_bounding_boxes\n",
    "\n",
    "\n",
    "# Some Neural Network Parameters:\n",
    "AUG_DIM = 9\n",
    "MAX_POINTS_PER_PILLAR = 100\n",
    "MAX_FILLED_PILLARS = 12000\n",
    "X_MIN = 0.0\n",
    "X_MAX = 70.4\n",
    "Y_MIN = -40.0\n",
    "Y_MAX = 40.0\n",
    "Z_MIN = -3.0\n",
    "Z_MAX = 1.0\n",
    "PILLAR_SIZE = (0.16, 0.16)\n",
    "#DESIRED_CLASSES = ['Car'] # More classes can be added here DEPRECATE?\n",
    "SCALE_FACTOR = 1.5\n",
    "H = 500\n",
    "W = 440\n",
    "\n",
    "\n",
    "ANCHORS = torch.tensor([[3.9, 1.6, 1.56, -1, 0], # Anchors as tensor: (height, width, height, z_center, orientation)\n",
    "                       [1.6, 3.9, 1.56, -1, 1.5708],\n",
    "                       [0.8, 0.6, 1.73, -0.6, 0],\n",
    "                       [0.6, 0.8, 1.73, -0.6, 1.5708]]\n",
    "                       )\n",
    "\n",
    "mapped_anchors = ANCHORS.detach().clone()\n",
    "mapped_anchors[:,0:2] /= PILLAR_SIZE[0]\n",
    "\n",
    "\n",
    "# Define a dictionary to map attributes to their indices\n",
    "attributes_idx = {\n",
    "    'norm_x': 7,\n",
    "    'norm_y': 8,\n",
    "    'norm_z': 9,\n",
    "    'norm_h': 10,\n",
    "    'norm_w': 11,\n",
    "    'norm_l': 12,\n",
    "}\n",
    "\n",
    "for anchor_tensor in mapped_anchors: # NOTE: This is regardless of the batch, it is for all the training and testing\n",
    "    anchor = Anchor(width=anchor_tensor[1], height=anchor_tensor[0])\n",
    "    anchor.create_anchor_grid(H,W) # Creates grid\n",
    "    anchor.create_anchors()\n",
    "    break # FIXME: Get rid of this\n",
    "    #anchors_list.append(anchor)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create data loaders'''\n",
    "\n",
    "train_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/velodyne_reduced'\n",
    "train_labels_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/label_2'\n",
    "\n",
    "small_train_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/small_train_velodyne'\n",
    "small_train_labels_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/small_labels_velodyne'\n",
    "\n",
    "mini_train_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/mini_train_velodyne'\n",
    "mini_train_labels_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/mini_label_velodyne'\n",
    "\n",
    "small_validation_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/small_validation_velodyne'\n",
    "small_validation_labels_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/small_validation_small_validation_labels'\n",
    "\n",
    "mini_validation_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/mini_validation_velodyne'\n",
    "mini_validation_labels_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/training/mini_validation_labels'\n",
    "\n",
    "\n",
    "test_pointclouds_dir = '/home/adlink/Documents/ECE-57000/ClassProject/Candidate2/PointPillars/dataset/kitti/testing/velodyne_reduced'\n",
    "\n",
    "\n",
    "# IMPORTANT: Set to CPU for pillarization otherwise, expect GPU memory to overflow\n",
    "device =  torch.device('cpu')\n",
    "\n",
    "\n",
    "# Create a collate function to handle variable-sized labels:\n",
    "def collate_batch(batch):\n",
    "    point_clouds, annotations = zip(*batch)\n",
    "    point_clouds = torch.stack(point_clouds, dim=0)\n",
    "    normalized_annotations = normalize_annotations(annotations, pillar_size=PILLAR_SIZE,\n",
    "        x_lims=(X_MIN, X_MAX), y_lims=(Y_MIN, Y_MAX))\n",
    "    \n",
    "    return point_clouds, normalized_annotations\n",
    "\n",
    "\n",
    "train_set = KITTIDataset(pointcloud_dir=mini_train_pointclouds_dir, labels_dir=mini_train_labels_dir)\n",
    "val_set = KITTIDataset(pointcloud_dir=mini_validation_pointclouds_dir, labels_dir=mini_validation_labels_dir)\n",
    "        \n",
    "# Create the dataset and DataLoader\n",
    "train_dataset = PseudoImageDataset(pointcloud_dir=mini_train_pointclouds_dir, device=device, kitti_dataset=train_set, aug_dim=AUG_DIM, max_points_in_pillar=MAX_POINTS_PER_PILLAR,\n",
    "                             max_pillars=MAX_FILLED_PILLARS, x_min=X_MIN, y_min=Y_MIN, z_min=Z_MIN, x_max = X_MAX, y_max=Y_MAX,\n",
    "                             z_max = Z_MAX, pillar_size=PILLAR_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "val_dataset = PseudoImageDataset(pointcloud_dir=mini_validation_pointclouds_dir, device=device, kitti_dataset=train_set, aug_dim=AUG_DIM, max_points_in_pillar=MAX_POINTS_PER_PILLAR,\n",
    "                             max_pillars=MAX_FILLED_PILLARS, x_min=X_MIN, y_min=Y_MIN, z_min=Z_MIN, x_max = X_MAX, y_max=Y_MAX,\n",
    "                             z_max = Z_MAX, pillar_size=PILLAR_SIZE)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: 500, second element: 440\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/Pipelines/kitti_loader.py:50: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return torch.from_numpy(point_cloud)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 0, loss: 33.3841438293457, elapsed: 13.612692594528198\n",
      "    Data prep time: 0.17s\n",
      "    Forward pass time: 6.60s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 6.82s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 1\n",
      "Epoch: 1, batch: 0, loss: 29.059093475341797, elapsed: 11.722165822982788\n",
      "    Data prep time: 0.14s\n",
      "    Forward pass time: 3.42s\n",
      "    Loss computation time: 0.00s\n",
      "    Backpropagation time: 8.15s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 40.104515075683594\n",
      "Epoch: 2\n",
      "Epoch: 2, batch: 0, loss: 26.092708587646484, elapsed: 10.974036693572998\n",
      "    Data prep time: 0.14s\n",
      "    Forward pass time: 3.65s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 7.17s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 3\n",
      "Epoch: 3, batch: 0, loss: 25.1231632232666, elapsed: 15.333303213119507\n",
      "    Data prep time: 0.22s\n",
      "    Forward pass time: 4.64s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 10.47s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 36.75337219238281\n",
      "Epoch: 4\n",
      "Epoch: 4, batch: 0, loss: 24.597631454467773, elapsed: 10.483127117156982\n",
      "    Data prep time: 0.15s\n",
      "    Forward pass time: 3.53s\n",
      "    Loss computation time: 0.00s\n",
      "    Backpropagation time: 6.80s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 5\n",
      "Epoch: 5, batch: 0, loss: 24.205307006835938, elapsed: 14.551640033721924\n",
      "    Data prep time: 0.27s\n",
      "    Forward pass time: 5.22s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 9.06s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 32.94472885131836\n",
      "Epoch: 6\n",
      "Epoch: 6, batch: 0, loss: 23.86283302307129, elapsed: 9.444956064224243\n",
      "    Data prep time: 0.14s\n",
      "    Forward pass time: 3.33s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 5.97s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 7\n",
      "Epoch: 7, batch: 0, loss: 23.54456901550293, elapsed: 16.11605739593506\n",
      "    Data prep time: 0.16s\n",
      "    Forward pass time: 5.27s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 10.68s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 30.91872787475586\n",
      "Epoch: 8\n",
      "Epoch: 8, batch: 0, loss: 23.267515182495117, elapsed: 13.111305952072144\n",
      "    Data prep time: 0.15s\n",
      "    Forward pass time: 3.68s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 9.27s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 9\n",
      "Epoch: 9, batch: 0, loss: 23.023544311523438, elapsed: 11.873482942581177\n",
      "    Data prep time: 0.31s\n",
      "    Forward pass time: 4.41s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 7.15s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 28.460620880126953\n",
      "Epoch: 10\n",
      "Epoch: 10, batch: 0, loss: 22.809492111206055, elapsed: 13.183083772659302\n",
      "    Data prep time: 0.23s\n",
      "    Forward pass time: 4.04s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 8.90s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 11\n",
      "Epoch: 11, batch: 0, loss: 22.612804412841797, elapsed: 10.214788675308228\n",
      "    Data prep time: 0.14s\n",
      "    Forward pass time: 3.78s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 6.29s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 26.970258712768555\n",
      "Epoch: 12\n",
      "Epoch: 12, batch: 0, loss: 22.42568016052246, elapsed: 14.796677589416504\n",
      "    Data prep time: 0.16s\n",
      "    Forward pass time: 5.53s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 9.10s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 13\n",
      "Epoch: 13, batch: 0, loss: 22.242462158203125, elapsed: 9.953839778900146\n",
      "    Data prep time: 0.14s\n",
      "    Forward pass time: 3.51s\n",
      "    Loss computation time: 0.00s\n",
      "    Backpropagation time: 6.30s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 26.100955963134766\n",
      "Epoch: 14\n",
      "Epoch: 14, batch: 0, loss: 22.063838958740234, elapsed: 12.98894476890564\n",
      "    Data prep time: 0.17s\n",
      "    Forward pass time: 5.55s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 7.26s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 15\n",
      "Epoch: 15, batch: 0, loss: 21.89310073852539, elapsed: 12.143074750900269\n",
      "    Data prep time: 0.16s\n",
      "    Forward pass time: 3.90s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 8.08s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "WARNING: Entering evaluation mode\n",
      "Validating with batch 0, got loss: 25.49420738220215\n",
      "Epoch: 16\n",
      "Epoch: 16, batch: 0, loss: 21.729578018188477, elapsed: 10.532896518707275\n",
      "    Data prep time: 0.15s\n",
      "    Forward pass time: 3.54s\n",
      "    Loss computation time: 0.01s\n",
      "    Backpropagation time: 6.84s\n",
      "Model saved to /home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/model_checkpoint.pth\n",
      "Epoch: 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m forward_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Refresh gradients for forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loc, size, clf, occupancy, angle, heading \u001b[39m=\u001b[39m model(pseudo_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m forward_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/train.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Loss:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/Pipelines/network.py:20\u001b[0m, in \u001b[0;36mPointPillarsModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# Forward pass through backbone and detection head\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     21\u001b[0m     loc, size, clf, occupancy, angle, heading \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetection_head(features)\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m loc, size, clf, occupancy, angle, heading\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/Pipelines/backbone.py:72\u001b[0m, in \u001b[0;36mBackBone.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m# Upsample and concatenate\u001b[39;00m\n\u001b[1;32m     71\u001b[0m up_x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup1(x1)   \n\u001b[0;32m---> 72\u001b[0m up_x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup2(x2)\n\u001b[1;32m     73\u001b[0m up_x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup3(x3)     \n\u001b[1;32m     74\u001b[0m concat_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([up_x1, up_x2, up_x3], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/Pipelines/backbone.py:45\u001b[0m, in \u001b[0;36mUpSample.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "model = PointPillarsModel(device=torch.device('cuda'))\n",
    "loss_fn = PointPillarLoss(feature_map_size=(H, W))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss = 0.0\n",
    "\n",
    "# Define a path to save the model\n",
    "model_save_path = \"/home/adlink/Documents/ECE-57000/ClassProject/github/PointPillars/Implementation/saved_models/dummy_model_checkpoint.pth\"\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    print(f'Epoch: {epoch}')    \n",
    "    \n",
    "    for batch_idx, (pseudo_images, batched_labels) in enumerate(train_loader):\n",
    "        \n",
    "        model.train()\n",
    "        # Start timer:\n",
    "        start_time = time.time()\n",
    "\n",
    "        data_prep_start = time.time()\n",
    "        gt_boxes_tensor = create_boxes_tensor(batched_labels, attributes_idx)\n",
    "        \n",
    "        # Check if gt_boxes_tensor is empty for the current batch\n",
    "        if gt_boxes_tensor.nelement() == 0:\n",
    "            print(f'Encountered an empty element on the batch')\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Get IoU tensor and regression targets:\n",
    "        iou_tensor = anchor.calculate_batch_iou(gt_boxes_tensor) \n",
    "        '''IoU tensor (batch_size, n_boxes, num_anchors_x, num_anchors_y)'''\n",
    "\n",
    "        # Regression targets from ground truth labels\n",
    "        regression_targets_tensor = anchor.get_regression_targets_tensor(iou_tensor, (H,W), threshold=0.5)\n",
    "\n",
    "        # Classification targets:\n",
    "        classification_targets_dict = anchor.get_classification_targets(iou_tensor=iou_tensor, feature_map_size=(H,W),\n",
    "                                    background_lower_threshold=0.05, background_upper_threshold=0.25)\n",
    "        \n",
    "        data_prep_end = time.time()\n",
    "\n",
    "\n",
    "        forward_start = time.time()\n",
    "        optimizer.zero_grad() # Refresh gradients for forward pass\n",
    "\n",
    "        loc, size, clf, occupancy, angle, heading = model(pseudo_images)\n",
    "        forward_end = time.time()\n",
    "        \n",
    "        # Loss:\n",
    "        loss_compute_start = time.time()\n",
    "        loss = loss_fn(regression_targets=regression_targets_tensor, classification_targets_dict=classification_targets_dict,\n",
    "        gt_boxes_tensor = gt_boxes_tensor, loc=loc, size=size, clf=clf, occupancy=occupancy, angle=angle, heading=heading,\n",
    "        anchor=anchor)\n",
    "        loss_compute_end = time.time()\n",
    "\n",
    "        # Backpropagation\n",
    "        backprop_start = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backprop_end = time.time()\n",
    "\n",
    "        end_time = time.time()  # End time of the batch\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        \n",
    "\n",
    "        # Print times and logging:\n",
    "        print(f'Epoch: {epoch}, batch: {batch_idx}, loss: {loss}, elapsed: {elapsed_time}')\n",
    "        print(f'    Data prep time: {data_prep_end - data_prep_start:.2f}s')\n",
    "        print(f'    Forward pass time: {forward_end - forward_start:.2f}s')\n",
    "        print(f'    Loss computation time: {loss_compute_end - loss_compute_start:.2f}s')\n",
    "        print(f'    Backpropagation time: {backprop_end - backprop_start:.2f}s')\n",
    "\n",
    "        # Save model every 15 batches:\n",
    "        if batch_idx % 15 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "        \n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            continue\n",
    "\n",
    "        # TODO: Add validation here:\n",
    "        model.eval()\n",
    "        print(f'WARNING: Entering evaluation mode')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx_val, (pseudo_images_val, batched_labels_val) in enumerate(val_loader):\n",
    "\n",
    "                if (batch_idx >= 5): # Break after 5 batches to avoid going over the full validation set\n",
    "                    break\n",
    "\n",
    "                gt_boxes_tensor_val = create_boxes_tensor(batched_labels_val, attributes_idx)\n",
    "        \n",
    "                # Check if gt_boxes_tensor is empty for the current batch\n",
    "                if gt_boxes_tensor_val.nelement() == 0:\n",
    "                    print(f'Encountered an empty element on the batch')\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Get IoU tensor and regression targets:\n",
    "                iou_tensor_val = anchor.calculate_batch_iou(gt_boxes_tensor_val) \n",
    "                '''IoU tensor (batch_size, n_boxes, num_anchors_x, num_anchors_y)'''\n",
    "\n",
    "                # Regression targets from ground truth labels\n",
    "                regression_targets_tensor_val = anchor.get_regression_targets_tensor(iou_tensor_val, (H,W), threshold=0.5)\n",
    "\n",
    "                # Classification targets:\n",
    "                classification_targets_dict_val = anchor.get_classification_targets(iou_tensor=iou_tensor_val, feature_map_size=(H,W),\n",
    "                                            background_lower_threshold=0.05, background_upper_threshold=0.25)\n",
    "                \n",
    "                loc_val, size_val, clf_val, occupancy_val, angle_val, heading_val = model(pseudo_images_val)\n",
    "    \n",
    "                loss_val = loss_fn(regression_targets=regression_targets_tensor_val, classification_targets_dict=classification_targets_dict_val,\n",
    "                gt_boxes_tensor = gt_boxes_tensor_val, loc=loc_val, size=size_val, clf=clf_val, \n",
    "                occupancy=occupancy_val, angle=angle_val, heading=heading_val, anchor=anchor)\n",
    "\n",
    "                print(f'Validating with batch {batch_idx_val}, got loss: {loss_val}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "model = ...  # Your complete PointPillars model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = PointPillarLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.8)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_data in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform pillarization and forward pass through the model\n",
    "        pillars, coords, ... = pillarize(batch_data)\n",
    "        loc, size, clf, ... = model(pillars, coords)\n",
    "        \n",
    "        # Generate targets using your Anchor class\n",
    "        regression_targets, classification_targets = anchor.generate_targets(batch_data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(regression_targets, classification_targets, batch_data['gt_boxes'], loc, size, clf, ...)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
